{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('learn_regression').master('local[1]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('file:///home/ffzs/python-projects/learn_spark/boston/train.csv', header=True, inferSchema=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.csv('file:///home/ffzs/python-projects/learn_spark/boston/test.csv', header=True, inferSchema=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat| medv|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98| 24.0|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14| 21.6|\n",
      "|  3|0.02729| 0.0| 7.07|   0|0.469|7.185|61.1|4.9671|  2|242|   17.8|392.83| 4.03|22.77|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_test = df_test.withColumn('medv', lit(22.77))\n",
    "df0 = df_train.union(df_test).sort('ID')\n",
    "df0.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "CRIM--  城镇人均犯罪率。\n",
    "ZN  - 占地面积超过25,000平方英尺的住宅用地比例。\n",
    "INDUS  - 每个城镇非零售业务的比例。\n",
    "CHAS  - Charles River虚拟变量（如果河流经过则= 1;否则为0）。\n",
    "NOX  - 氮氧化物浓度（每千万份）。\n",
    "RM  - 每间住宅的平均房间数。\n",
    "AGE  - 1940年以前建造的自住单位比例。\n",
    "DIS  - 加权平均值到五个波士顿就业中心的距离。\n",
    "RAD  - 径向高速公路的可达性指数。\n",
    "TAX  - 每10,000美元的全额物业税率。\n",
    "PTRATIO  - 城镇的学生与教师比例。\n",
    "BLACK  - 1000（Bk - 0.63）²其中Bk是城镇黑人的比例。\n",
    "LSTAT  - 人口较低的地位（百分比）。\n",
    "MEDV  - 自住房屋的中位数价值1000美元。这是目标变量。\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "def feature_converter(df):\n",
    "    vecAss = VectorAssembler(inputCols=df0.columns[1:-1], outputCol='features')\n",
    "    df_va = vecAss.transform(df)\n",
    "    return df_va\n",
    "\n",
    "train_data, test_data = feature_converter(df0).select(['features', 'medv']).randomSplit([7.0, 3.0], 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树回归\n",
    "`pyspark.ml.regression.DecisionTreeRegressor(featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='variance', seed=None, varianceCol=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "fit(dataset, params=None)方法 \n",
    "Impurity: 信息增益计算准则，支持选项：variance \n",
    "maxBins: 连续特征离散化的最大分箱个数， >=2并且>=任何分类特征的分类个数 \n",
    "maxDepth: 最大树深 \n",
    "minInfoGain: 分割节点所需最小信息增益 \n",
    "minInstancesPerNode: 分割后每个子节点最小实例个数 \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(maxDepth=5, varianceCol=\"variance\", labelCol='medv')\n",
    "dt_model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(13, {0: 0.0503, 2: 0.011, 4: 0.0622, 5: 0.1441, 6: 0.1852, 7: 0.0262, 8: 0.0022, 9: 0.0886, 10: 0.0142, 12: 0.4159})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+------------------+\n",
      "|            features| medv|        prediction|          variance|\n",
      "+--------------------+-----+------------------+------------------+\n",
      "|[0.03237,0.0,2.18...| 33.4| 34.12833333333334|29.509013888888756|\n",
      "|[0.08829,12.5,7.8...| 22.9|21.195135135135136| 4.446162819576342|\n",
      "|[0.14455,12.5,7.8...|22.77|22.425999999999995|0.5578440000003866|\n",
      "+--------------------+-----+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据的均方根误差（rmse）:6.555920141221407\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "dt_evaluator = RegressionEvaluator(labelCol='medv', metricName=\"rmse\", predictionCol='prediction')\n",
    "rmse = dt_evaluator.evaluate(result)\n",
    "print('测试数据的均方根误差（rmse）:{}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度提升树回归 （Gradient-boosted tree regression）\n",
    "pyspark.ml.regression.GBTRegressor(featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, subsamplingRate=1.0, checkpointInterval=10, lossType='squared', maxIter=20, stepSize=0.1, seed=None, impurity='variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "fit(dataset,params=None)方法 \n",
    "lossType: GBT要最小化的损失函数，可选：squared, absolute  \n",
    "maxIter: 最大迭代次数 \n",
    "stepSize: 每次优化迭代的步长 \n",
    "subsamplingRate:用于训练每颗决策树的训练数据集的比例，区间[0,1] \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(maxIter=10, labelCol='medv', maxDepth=3)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(13, {0: 0.0219, 1: 0.0364, 2: 0.0305, 3: 0.0114, 4: 0.0032, 5: 0.1372, 6: 0.146, 7: 0.1033, 8: 0.0518, 9: 0.0819, 10: 0.0883, 11: 0.0048, 12: 0.2832})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features| medv|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[0.03237,0.0,2.18...| 33.4| 31.98716729056085|\n",
      "|[0.08829,12.5,7.8...| 22.9|22.254258637918248|\n",
      "|[0.14455,12.5,7.8...|22.77|20.066468254729102|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.treeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据的均方根误差（rmse）:5.624145397622545\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "gbt_evaluator = RegressionEvaluator(labelCol='medv', metricName=\"rmse\", predictionCol='prediction')\n",
    "rmse = gbt_evaluator.evaluate(result)\n",
    "print('测试数据的均方根误差（rmse）:{}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归（LinearRegression）\n",
    "pyspark.ml.regression.LinearRegression(featuresCol='features', labelCol='label', predictionCol='prediction', maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-06, fitIntercept=True, standardization=True, solver='auto', weightCol=None, aggregationDepth=2, loss='squaredError', epsilon=1.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "学习目标是通过正规化最小化指定的损失函数。这支持两种损失：\n",
    "+ squaredError (a.k.a 平方损失)\n",
    "+ huber (对于相对较小的误差和相对大的误差的绝对误差的平方误差的混合，我们从训练数据估计比例参数)\n",
    "\n",
    "支持多种类型的正则化： \n",
    "+ None：OLS \n",
    "+ L2：ridge回归 \n",
    "+ L1：Lasso回归 \n",
    "+ L1+L2：elastic回归\n",
    "\n",
    "注意：与huber loss匹配仅支持none和L2正规化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "aggregationDepth: 树聚合的深度, >=2 \n",
    "elasticNtParam: ElasticNet混合参数，在[0,1]范围内，alpha=0为L2， alpha=1为L1 \n",
    "fit(dataset,params=None)方法 \n",
    "fitIntercept: 是否拟合截距 \n",
    "maxIter: 最大迭代次数 \n",
    "regParam：正则化参数 >=0 \n",
    "solver: 优化算法，没设置或空则使用”auto” \n",
    "standardization: 是否对拟合模型的特征进行标准化 \n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "Summary属性\n",
    "coefficientStandardErrors \n",
    "devianceResiduals: 加权残差 \n",
    "explainedVariance: 返回解释的方差回归得分，explainedVariance=1−variance(y−(̂ y))/variance(y) \n",
    "meanAbsoluteError: 返回均值绝对误差 \n",
    "meanSquaredError: 返回均值平方误 \n",
    "numInstances: 预测的实例个数 \n",
    "pValues: 系数和截距的双边P值，只有用”normal”solver才可用 \n",
    "predictions: 模型transform方法返回的预测 \n",
    "r2: R方 \n",
    "residuals: 残差 \n",
    "rootMeanSquaredError: 均方误差平方根 \n",
    "tValues： T统计量\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10, elasticNetParam=0.8, regParam=0.3, labelCol='medv')\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.457496\n",
      "r2: 0.432071\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features| medv|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[0.03237,0.0,2.18...| 33.4|27.066314856077966|\n",
      "|[0.08829,12.5,7.8...| 22.9|23.721352298735898|\n",
      "|[0.14455,12.5,7.8...|22.77|21.388248900632398|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = lr_model.transform(test_data)\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R平方（r2）:0.469\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(labelCol='medv', metricName=\"r2\", predictionCol='prediction')\n",
    "r2 = lr_evaluator.evaluate(result)\n",
    "print('R平方（r2）:{:.3}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_evaluation = lr_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:5.7\n",
      "r2:0.469\n"
     ]
    }
   ],
   "source": [
    "print('RMSE:{:.3}'.format(test_evaluation.rootMeanSquaredError))\n",
    "print('r2:{:.3}'.format(test_evaluation.r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林回归\n",
    "pyspark.ml.regression.RandomForestRegressor(featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='variance', subsamplingRate=1.0, seed=None, numTrees=20, featureSubsetStrategy='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "fit(dataset,params=None)方法 \n",
    "featureSubsetStrategy: 每棵树的节点上要分割的特征数量，可选：auto, all, onethird, sqrt, log2,(0.0,1.0],[1-n] \n",
    "impurity: 信息增益计算的准则，可选：variance \n",
    "maxBins: 连续特征离散化最大分箱个数。 \n",
    "maxDepth: 树的最大深度 \n",
    "minInfoGain: 树节点分割特征所需最小的信息增益 \n",
    "minInstancesPerNode: 每个结点所需最小实例个数 \n",
    "numTrees: 训练树的个数 \n",
    "subsamplingRate: 学习每颗决策树所需样本比例 \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(numTrees=10, maxDepth=5, seed=101, labelCol='medv')\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features| medv|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[0.03237,0.0,2.18...| 33.4| 30.12804440796982|\n",
      "|[0.08829,12.5,7.8...| 22.9|21.338106353716338|\n",
      "|[0.14455,12.5,7.8...|22.77|19.764914032872827|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rf_model.transform(test_data)\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.treeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据的均方根误差（rmse）:5.268739233773331\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "rf_evaluator = RegressionEvaluator(labelCol='medv', metricName=\"rmse\", predictionCol='prediction')\n",
    "rmse = rf_evaluator.evaluate(result)\n",
    "print('测试数据的均方根误差（rmse）:{}'.format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
